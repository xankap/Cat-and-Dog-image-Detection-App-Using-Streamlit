# -*- coding: utf-8 -*-
"""Training notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cEzMGd04LPNY617dIqexwl9_4xGTqBq-
"""



!pip install tensorflow streamlit pillow numpy

# download dataset

!pip3 install opendatasets

import opendatasets as od

!wget https://www.kaggle.com/c/dogs-vs-cats/train.zip
!wget https://www.kaggle.com/c/dogs-vs-cats/test1.zip

od.download("https://www.kaggle.com/competitions/dogs-vs-cats/data")

# unzip dataset

!unzip /content/dogs-vs-cats/train.zip
!unzip /content/dogs-vs-cats/test1.zip

#put the file in directories
base_dir = "/content/dogs-vs-cats"
train_dir = "/content/train"
test_dir = "/content/test1"

import glob
import os, sys, zipfile

train_list = glob.glob(os.path.join(train_dir, "*.jpg"))
test_list = glob.glob(os.path.join(test_dir, "*.jpg"))



import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import os
import shutil
import random

# Image parameters
IMG_SIZE = 150
BATCH_SIZE = 32

# --- Start of fix: Data restructuring ---
original_train_dir = "/content/train"
original_test_dir = "/content/test1" # This directory is for test predictions, not for model validation

# Define paths for structured dataset
base_processed_dir = '/content/structured_cats_dogs'
train_processed_dir = os.path.join(base_processed_dir, 'train')
validation_processed_dir = os.path.join(base_processed_dir, 'validation')

# Remove existing structured directories if they exist (for rerunning cell)
if os.path.exists(base_processed_dir):
    print(f"Removing existing directory: {base_processed_dir}")
    shutil.rmtree(base_processed_dir)

# Create base directories
print(f"Creating directory: {train_processed_dir}")
os.makedirs(train_processed_dir)
print(f"Creating directory: {validation_processed_dir}")
os.makedirs(validation_processed_dir)

# Create class subdirectories for training
train_cats_dir = os.path.join(train_processed_dir, 'cats')
train_dogs_dir = os.path.join(train_processed_dir, 'dogs')
print(f"Creating directory: {train_cats_dir}")
os.makedirs(train_cats_dir)
print(f"Creating directory: {train_dogs_dir}")
os.makedirs(train_dogs_dir)

# Create class subdirectories for validation
validation_cats_dir = os.path.join(validation_processed_dir, 'cats')
validation_dogs_dir = os.path.join(validation_processed_dir, 'dogs')
print(f"Creating directory: {validation_cats_dir}")
os.makedirs(validation_cats_dir)
print(f"Creating directory: {validation_dogs_dir}")
os.makedirs(validation_dogs_dir)

# Get lists of existing images from the original train directory
all_train_images = [f for f in os.listdir(original_train_dir) if f.endswith('.jpg')]
print(f"Found {len(all_train_images)} total images in {original_train_dir}")

cats_files = [f for f in all_train_images if 'cat' in f.lower()] # Added .lower() for robustness
dogs_files = [f for f in all_train_images if 'dog' in f.lower()] # Added .lower() for robustness

print(f"Found {len(cats_files)} cat images and {len(dogs_files)} dog images.")

# Shuffle and split files for training and validation
random.seed(42) # for reproducibility
random.shuffle(cats_files)
random.shuffle(dogs_files)

# 80% for training, 20% for validation
train_split_ratio = 0.8

num_train_cats = int(len(cats_files) * train_split_ratio)
num_validation_cats = len(cats_files) - num_train_cats

num_train_dogs = int(len(dogs_files) * train_split_ratio)
num_validation_dogs = len(dogs_files) - num_train_dogs

print(f"Splitting: Train Cats: {num_train_cats}, Validation Cats: {num_validation_cats}")
print(f"Splitting: Train Dogs: {num_train_dogs}, Validation Dogs: {num_validation_dogs}")

# Assign files to their respective sets
train_cats_set = cats_files[:num_train_cats]
validation_cats_set = cats_files[num_train_cats:]

train_dogs_set = dogs_files[:num_train_dogs]
validation_dogs_set = dogs_files[num_train_dogs:]

# Function to move files
def move_image_files(file_list, source_dir, destination_dir):
    moved_count = 0
    for fname in file_list:
        src = os.path.join(source_dir, fname)
        dst = os.path.join(destination_dir, fname)
        try:
            shutil.copyfile(src, dst) # Use copyfile to keep original data untouched
            moved_count += 1
        except FileNotFoundError:
            print(f"Warning: File not found during copy: {src}")
    return moved_count

# Move images to the new structured directories
print("Moving training cat images...")
moved_train_cats = move_image_files(train_cats_set, original_train_dir, train_cats_dir)
print(f"Moved {moved_train_cats} training cat images.")

print("Moving training dog images...")
moved_train_dogs = move_image_files(train_dogs_set, original_train_dir, train_dogs_dir)
print(f"Moved {moved_train_dogs} training dog images.")

print("Moving validation cat images...")
moved_validation_cats = move_image_files(validation_cats_set, original_train_dir, validation_cats_dir)
print(f"Moved {moved_validation_cats} validation cat images.")

print("Moving validation dog images...")
moved_validation_dogs = move_image_files(validation_dogs_set, original_train_dir, validation_dogs_dir)
print(f"Moved {moved_validation_dogs} validation dog images.")

# --- End of fix: Data restructuring ---

train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(
    train_processed_dir, # Use the new structured training directory
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="binary"
)

val_data = val_datagen.flow_from_directory(
    validation_processed_dir, # Use the new structured validation directory
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="binary"
)

# CNN Model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.fit(
    train_data,
    epochs=10,
    validation_data=val_data
)

model.save("cats_dogs_model.h5")
print("Model saved successfully!")

import os

# Define the base directory for the structured dataset
base_processed_dir = '/content/structured_cats_dogs'

# List the contents of the structured dataset directory
print(f"Listing contents of {base_processed_dir}:")

# Use os.walk to get a more detailed view of the directory structure
for dirpath, dirnames, filenames in os.walk(base_processed_dir):
    # Calculate current depth for indentation
    depth = dirpath.replace(base_processed_dir, '').count(os.sep)
    indent = '  ' * depth

    # Print current directory
    print(f"{indent}[{os.path.basename(dirpath)}]")

    # Print subdirectories
    for dname in dirnames:
        print(f"{indent}  |- {dname}/")
    # Print first 5 files if there are any, and total count
    if filenames:
        print(f"{indent}  |- ... ({len(filenames)} files)")
        for fname in sorted(filenames)[:5]: # Print a few sample files
            print(f"{indent}  |   |- {fname}")


print("\nVerification complete.")